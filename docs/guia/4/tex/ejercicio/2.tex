\indent\underline{\textbf{Ejercicio 2}}\\
Demostrar que la política $\varepsilon \text{ - \textit{Greedy}}$, definida de la siguiente manera

\begin{equation}
    \pi(a|s) = \left\{
    \begin{array}{lcc}
        1 - \varepsilon + \frac{\varepsilon}{|A(s)|} & si & a = a^{\ast}    \\ \\
        \frac{\varepsilon}{|A(s)|} & si & a \neq a^{\ast}
    \end{array}
    \right.\label{eq:equation}
\end{equation}

es una distribución de probabilidad válida, donde $|A(s)|$ es el número de acciones para el estado $s, \ \varepsilon < 1$ es un número positivo pequeño, y $a^{\ast}$ es la acción óptima (decisión greedy) para el estado $s$.
¿Hay que pedir alguna condición sobre $\varepsilon$?

\indent\underline{\textbf{Solución}}\\

Para la demostración en cuestión, la política $\pi(a|s)$ es:

\begin{equation}
    \pi(a|s) = \left\{
    \begin{array}{lcc}
        1 - \varepsilon + \frac{\varepsilon}{|A(s)|} & si & a = a^{\ast}    \\ \\
        \frac{\varepsilon}{|A(s)|} & si & a \neq a^{\ast}
    \end{array}
    \right.
    \label{eq:equation2}
\end{equation}

\paragraph{$a=a^{\ast}$}

La probabilidad corresponde a:

\[
    \pi \left(a^{\ast} \mid s\right) = 1 - \varepsilon + \frac{\varepsilon}{|A(s)|}
\]

Considerando que $0 \leq \varepsilon < 1$ y $\frac{\varepsilon}{|A(s)|} \geq 0$, se tiene que:

\[
    \pi \left(a^{\ast} \mid s\right) \geq 1 - \varepsilon > 0
\]

\paragraph{$a \neq a^{\ast}$}

La probabilidad corresponde a:

\[
    \pi \left(a \mid s\right) = \frac{\varepsilon}{|A(s)|}
\]

Considerando que $\varepsilon \geq 0$ y $|A(s)| > 0$, se tiene que:

\[
    \pi \left(a \mid s\right) \geq 0
\]

Por lo tanto, la política $\pi(a|s) \geq 0$ para toda acción.

\paragraph{Suma de probabilidades}

La suma de las probabilidades para todas las acciones es:

\[
    \sum_{a \in A(s)} \pi \left(a \mid s\right) = \pi \left( a^{\ast} \mid s \right)  + \sum_{a \neq a^{\ast}} \pi \left(a \mid s\right)
\]

Si $a = a^{\ast}$, entonces:

\[
    \pi \left(a^{\ast} \mid s\right) = 1 - \varepsilon + \frac{\varepsilon}{|A(s)|}
\]

Para $a \neq a^{\ast}$, existen $|A(s)| - 1$ acciones no óptimas, por lo que:


\[
    \sum_{a \neq a^{\ast}} \pi \left(a \mid s\right) = \left(|A(s)| - 1\right) \cdot \frac{\varepsilon}{|A(s)|}
\]

Por lo tanto, la suma de las probabilidades es:

\[
    \sum_{a \neq a^{\ast}} \pi \left(a \mid s\right) = 1 - \varepsilon + \frac{\varepsilon}{|A(s)|} + \left(|A(s)| - 1\right) \cdot \frac{\varepsilon}{|A(s)|}
\]

Al separar el término $\frac{\varepsilon}{|A(s)|}$, se tiene:

\begin{align*}
    \sum_{a \in A(s)} \pi \left(a \mid s\right) &= 1 - \varepsilon + \varepsilon \\
    \sum_{a \in A(s)} \pi \left(a \mid s\right) &= 1
\end{align*}

Por lo tanto, la política $\pi(a|s)$ es una distribución de probabilidad válida ya que suman $1$.

\paragraph{Condición sobre $\varepsilon$}

La condición sobre $\varepsilon$ es que $\varepsilon \geq 0$ y $\varepsilon < 1$.
De tal manera se garantiza:

\begin{itemize}
    \item Que la probabilidad de la acción óptima $a^{\ast}$, $1 - \varepsilon + \frac{\varepsilon}{|A(s)|}$ sea mayor a $0$.
    \item Las probabilidades de las acciones no óptimas, $\frac{\varepsilon}{|A(s)|}$, sean mayores a $0$.
\end{itemize}

\line(1,0){\textwidth}

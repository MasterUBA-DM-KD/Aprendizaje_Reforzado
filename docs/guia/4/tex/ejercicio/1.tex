\indent\underline{\textbf{Ejercicio 1}}\\
Considere un proceso de decisión de Markov (MDP) con dos estados: uno terminal y otro no-terminal.
Existe sólo una acción que lleva del estado no-terminal al estado terminal con probabilidad $1 - \rho$ y del estado no-terminal a si mismo con probabilidad $\rho$.
La recompensa es $+1$ en todas las transiciones y el
factor de descuento es $\gamma = 1$.
Suponga que observa un episodio con $10$ iteraciones y un retorno de $10$.

\begin{itemize}
    \item ¿Cuáles son las estimaciones Monte Carlo de primer-visita y de cada-visita del valor del estado no-terminal basadas en ese episodio?
    \item Compare los valores de estado obtenidos con el teórico si $\rho = 0.9$.
    Saque conclusiones.
\end{itemize}

\indent\underline{\textbf{Solución}}\\

\line(1,0){\textwidth}
